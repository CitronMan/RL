状态量较少，随机action可以到达terminal state.
对策略进行随机采样,得到环境对策略每个action的reward,更新状态价值。
状态价值更新策略：
对于策略pi:v(s) = E(y*R_i)
对于样本迭代：V_k(s) = V_{k-1}(s) + v(s)
最后V_k收敛，得到最后的状态价值

待续，要明确下，这个模型有关和无关的区别。

